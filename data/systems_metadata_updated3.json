{
  "AMI": {
    "constrained": true,
    "description": "The AMI submission for WMT 2025 is based on a pre-trained Llama 3.2 3B parameter language model. We pre-train the model with 10 billion tokens of English and Icelandic texts, fine-tune on parallel corpora and finally do contrastive preference optimization on the model. For the submission we produce multiple hypotheses with different temperatures on the model and use CometKiwi to select the final hypotheses.",
    "architecture": "LLM",
    "base_model": "LLama 3.2 3B",
    "parameter_count": "3",
    "translation_granularity": "sentence, paragraph, mixed",
    "post_editing": "Minimum Risk Bayes decoding, Quality estimation, Regular expressions that check for consistency in usage of emojis, hashtags, URLs and some punctuation in source text and translation.",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, contrastive finetuning (CPO, DPO)",
    "supported_lps": {
      "en-is_IS": "supported"
    }
  },
  "Yolu": {
    "constrained": true,
    "description": "This paper details Yolu's submission for the WMT'25 General Machine Translation Task. Our work, situated within the constrained track, investigates the efficacy of Reinforcement Learning (RL) in enhancing machine translation. Our system is built upon the open-source Qwen3 model. We introduce a robust methodology for continuous performance improvement, which combines meticulous data cleaning with advanced data distillation techniques. This is complemented by a multi-stage optimization strategy, sequentially employing Continued Pre-Training (CPT), Supervised Fine-Tuning (SFT), Contrastive Preference Optimization (CPO), and a novel policy optimization algorithm, Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO). Furthermore, we integrate a Quality Estimation (QE) model to facilitate online QE distillation, thereby refining the model's output during the decoding phase.",
    "architecture": "LLMs",
    "base_model": "qwen",
    "parameter_count": "14",
    "translation_granularity": "sentence",
    "post_editing": "Minimum Risk Bayes decoding, Automatic post editing (e.g. LLM), Prompt engineering",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-cs_CZ": "supported",
      "en-et_EE": "supported",
      "en-ja_JP": "supported",
      "en-ko_KR": "supported",
      "en-ru_RU": "supported",
      "en-sr_Latn_RS": "supported",
      "en-uk_UA": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "bb88": {
    "constrained": false,
    "description": "",
    "supported_lps": {
      "en-ja_JP": "unknown",
      "ja-zh_CN": "unknown"
    }
  },
  "SalamandraTA": {
    "constrained": true,
    "description": "In this paper, we present the \\salamandraTA\\ family of models, an improved iteration of \\salamandra\\ LLMs \\cite{gonzalezagirre2025salamandratechnicalreport} specifically trained to achieve strong performance in translation-related tasks for 38 European languages. \\salamandraTA\\ comes in two scales: 2B and 7B parameters. For both versions, we applied the same training recipe with a first step of continual pre-training on parallel data, and a second step of supervised fine-tuning on high-quality instructions.\r\n\r\nThe BSC submission to the WMT25 General Machine Translation shared task is based on the 7B variant of \\salamandraTA. We first extended the model vocabulary to support the seven additional non-European languages included in the task. This was followed by a second phase of continual pretraining and supervised fine-tuning, carefully designed to optimize performance across all translation directions for this year's shared task. For decoding, we employed two quality-aware strategies: Minimum Bayes Risk Decoding and Translation Reranking using \\comet\\ and \\cometkiwi.\r\n\r\nWe publicly release both the 2B and 7B versions of \\salamandraTA, along with the newer \\salamandraTAversiontwo\\ model, on Hugging Face.",
    "architecture": "LLMs",
    "base_model": "Salamandra7B",
    "parameter_count": "7.77",
    "translation_granularity": "sentence, paragraph, document (default in blindset)",
    "post_editing": "Minimum Risk Bayes decoding, Translation Re-ranking",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI)",
    "model_manipulation": "supervised finetuning, continual pre-training",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-uk_UA": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "CGFOKUS": {
    "constrained": false,
    "description": "",
    "architecture": "LLMs",
    "base_model": "Qwen3:235B",
    "parameter_count": "235",
    "translation_granularity": "paragraph",
    "post_editing": "Prompt engineering, simple automated formatting",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "The languages specified were use to select the used LLM among the largest ones that can be run locally",
    "supported_lps": {
      "en-uk_UA": "supported"
    }
  },
  "CommandA-MT": {
    "constrained": false,
    "description": "We built our system on top of Command-A using a direct preference optimization with data preparation pipeline that emphasizes robust data quality control, primarily incorporating standard quality filtering along with a novel difficulty filtering component, which serves as the key innovation of our approach. The final translation is built through step-by-step reasoning, and we employ Minimum Bayes Risk decoding with a candidate pool size of 20, using MetricX-XL as the primary utility metric. For unsupported languages, we use a second model prepared identically but with an additional initial supervised fine-tuning step for the unsupported languages that Command-A model has not seen.",
    "architecture": "LLM",
    "base_model": "CommandA",
    "parameter_count": "111B",
    "translation_granularity": "document (default in blindset)",
    "post_editing": "Minimum Risk Bayes decoding, Reasoning",
    "is_multimodal": "NO",
    "data_preparation": "More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "contrastive finetuning (CPO, DPO)",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "CUNI-SFT": {
    "constrained": true,
    "description": "CUNI-SFT submission is based on pretrained LLMs finetuned using LoRA on publicly available corpora to translate from English to Czech, Ukrainian and Serbian, and from Czech to Ukrainian. We experimented with EuroLLM 9B Instruct, Aya expanse 8B, IBM Granite 3.3 8B, and Mistral 7B Instruct v0.3 as the pretrained models and we finetuned them on the datasets from OPUS wrapped in different prompts for sentence level, sentece level with context awareness and paragraph level translation.",
    "architecture": "LLM",
    "base_model": "EuroLLM",
    "parameter_count": "9",
    "translation_granularity": "sentence, paragraph, document (default in blindset), mixed",
    "post_editing": "No post-editing, pure LLM output",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI)",
    "model_manipulation": "supervised finetuning, adaptions (e.g. LoRA)",
    "supported_lps": {
      "cs-uk_UA": "supported",
      "en-cs_CZ": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-uk_UA": "supported"
    }
  },
  "CUNI-DocTransformer": {
    "constrained": true,
    "description": "CUNI-DocTransformer is a standard enc-dec Transformer trained with Block backtranslation on multi-sentence sequences. It is the same system as in WMT20--WMT24.",
    "architecture": "enc-dec",
    "base_model": "from scratch",
    "parameter_count": "<1",
    "translation_granularity": "document (default in blindset), floating+overlapping window of several segments",
    "post_editing": "checkpoint-avg",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...)",
    "model_manipulation": "unknown",
    "supported_lps": {
      "en-cs_CZ": "supported"
    }
  },
  "CUNI-EdUKate-v1": {
    "constrained": false,
    "description": "CUNI-EdUKate-v1 is EuroLLM-9B-Instruct finetuned for Czech to Ukrainian machine translation using LoRA and CPO on data from educational domain.",
    "architecture": "LLM",
    "base_model": "utter-project/EuroLLM-9B-Instruct",
    "parameter_count": "9.35B",
    "translation_granularity": "sentence",
    "post_editing": "None (just greedy decode 0-shot LLM translation)",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "comments": "The model size is ~9.15B + ~0.204B single LoRA adapter",
    "supported_lps": {
      "cs-uk_UA": "supported"
    }
  },
  "CUNI-MH-v2": {
    "constrained": true,
    "description": "CUNI-MH-v2 is an EuroLLM-9B-Instruct finetuned for en2cs and cs2de machine translation using LoRA and CPO. We trained separate LoRA adapters for both languages. Both language pairs were trained on samples from publicly available CzEng 2.0 dataset.",
    "architecture": "LLM",
    "base_model": "utter-project/EuroLLM-9B-Instruct",
    "parameter_count": "9",
    "translation_granularity": "paragraph",
    "post_editing": "None, we just greedy decode 0-shot LLM translation",
    "is_multimodal": "NO",
    "data_preparation": "More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "comments": "Model parameter size is ~9.154B (base EuroLLM model) + 2xLoRA adapter (2x ~0.108B) = ~9.37B - note that only one LoRA adapter is active at a time - we use different one for each language pair and only do two pairs here.",
    "supported_lps": {
      "cs-de_DE": "supported",
      "en-cs_CZ": "supported"
    }
  },
  "CUNI-Transformer": {
    "constrained": true,
    "description": "CUNI-Transformer is a standard enc-dec Transformer trained with Block backtranslation, the same system as in WMT23 and WMT24.",
    "architecture": "enc-dec",
    "base_model": "trained from scratch",
    "parameter_count": "<1",
    "translation_granularity": "sentence",
    "post_editing": "checkpoint-avg",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), Synthetic data (back-translation, other)",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-uk_UA": "supported"
    }
  },
  "DLUT_GTCOM": {
    "constrained": false,
    "description": "DLUT and GTCOM Machine Translation Systems for WMT25,\r\nNMT+Gemma3 27B",
    "architecture": "LLMs",
    "base_model": "Gemma3-27B",
    "parameter_count": "27B",
    "translation_granularity": "sentence, paragraph",
    "post_editing": "Prompt engineering",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-uk_UA": "supported"
    }
  },
  "RuZh": {
    "constrained": true,
    "description": "Eole NLP Submission uses Tower+ 9B model with an extra layer for quality estimation.\r\n\r\nIt generates multiple hypotheses and rank them according to an internal score.",
    "architecture": "LLMs + Estimator",
    "base_model": "Tower+ 9B",
    "parameter_count": "9B",
    "translation_granularity": "paragraph",
    "post_editing": "Quality estimation",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "en-ru_RU": "unknown",
      "en-zh_CN": "unknown"
    }
  },
  "Algharb": {
    "constrained": true,
    "description": "In this submission, we present a sophisticated Large Language Model (LLM) architecture driven by a comprehensive training pipeline. Our approach features multi-stage translation supervised fine-tuning (SFT) and a two-phase post-training process, collectively producing substantial improvements in translation accuracy.\r\n\r\nTo address the shortcomings of conventional decoding strategies and bolster model stability, we incorporate word alignment techniques alongside an advanced MBR (Minimum Bayes Risk) plus reranking decoding algorithm. This integrated strategy yields more robust and consistent translation outputs across varied linguistic contexts.\r\n\r\nOur method delivers superior performance, achieving state-of-the-art results on both automatic Quality Estimation (QE) metrics and LLM-based judge evaluations. With its resilient architecture and innovative decoding procedures, the AllTrans system stands out as a leading solution in today’s translation landscape.",
    "architecture": "LLMs",
    "base_model": "Qwen3-14B",
    "parameter_count": "14B",
    "translation_granularity": "mixed",
    "post_editing": "Minimum Risk Bayes decoding, Quality estimation",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-cs_CZ": "supported",
      "en-et_EE": "supported",
      "en-ja_JP": "supported",
      "en-ko_KR": "supported",
      "en-ru_RU": "supported",
      "en-sr_Latn_RS": "supported",
      "en-uk_UA": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "GemTrans": {
    "constrained": false,
    "description": "",
    "architecture": "LLM",
    "base_model": "Gemma 3",
    "parameter_count": "27",
    "translation_granularity": "paragraph",
    "post_editing": "Automatic post editing (e.g. LLM)",
    "is_multimodal": "NO",
    "data_preparation": "Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, RL",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "SH": {
    "constrained": false,
    "description": "Team SH's submission consists of two standard LLMs based on cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese: an initial translation model fine-tuned with LoRA \\citep{hu2022lora}, and an automatic post-editing model fine-tuned with LoRA and SimPO \\citep{meng2024simpo}. Both use MBR decoding \\citep{muller-sennrich-2021-understanding}.\r\n\r\n@inproceedings{\r\nmeng2024simpo,\r\ntitle={Sim{PO}: Simple Preference Optimization with a Reference-Free Reward},\r\nauthor={Yu Meng and Mengzhou Xia and Danqi Chen},\r\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\r\nyear={2024},\r\nurl={https://openreview.net/forum?id=3Tzcot1LKb}\r\n}\r\n\r\n@inproceedings{\r\nhu2022lora,\r\ntitle={Lo{RA}: Low-Rank Adaptation of Large Language Models},\r\nauthor={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},\r\nbooktitle={International Conference on Learning Representations},\r\nyear={2022},\r\nurl={https://openreview.net/forum?id=nZeVKeeFYf9}\r\n}\r\n\r\n@inproceedings{muller-sennrich-2021-understanding,\r\n    title = \"Understanding the Properties of Minimum {B}ayes Risk Decoding in Neural Machine Translation\",\r\n    author = {M{\\\"u}ller, Mathias  and\r\n      Sennrich, Rico},\r\n    editor = \"Zong, Chengqing  and\r\n      Xia, Fei  and\r\n      Li, Wenjie  and\r\n      Navigli, Roberto\",\r\n    booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\",\r\n    month = aug,\r\n    year = \"2021\",\r\n    address = \"Online\",\r\n    publisher = \"Association for Computational Linguistics\",\r\n    url = \"https://aclanthology.org/2021.acl-long.22/\",\r\n    doi = \"10.18653/v1/2021.acl-long.22\",\r\n    pages = \"259--272\",\r\n}",
    "architecture": "LLMs",
    "base_model": "cyberagent/DeepSeek-R1-Distill-Qwen-14B-Japanese",
    "parameter_count": "4 * 14B",
    "translation_granularity": "sentence",
    "post_editing": "Minimum Risk Bayes decoding, Automatic post editing (e.g. LLM)",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "supervised finetuning, contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "supported_lps": {
      "en-ja_JP": "supported"
    }
  },
  "In2x": {
    "constrained": false,
    "description": "This paper presents the open-system submission by the In2x research team for the WMT25 General Machine Translation Shared Task. Our submission focuses on Japanese-related translation tasks, aiming to explore a generalizable paradigm for extending large language models (LLMs) to other languages. This paradigm encompasses aspects such as data construction methods and reward model design. The ultimate goal is to enable large language model systems to achieve exceptional performance in low-resource or less commonly spoken languages.",
    "architecture": "dec-only",
    "base_model": "qwen2.5_72b(continue pretraining)",
    "parameter_count": "72",
    "translation_granularity": "paragraph",
    "post_editing": "Minimum Risk Bayes decoding, Ensemble",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "en-ja_JP": "unknown",
      "ja-zh_CN": "unknown"
    }
  },
  "IRB-MT": {
    "constrained": true,
    "description": "IRB-MT submission is a simple agentic system based on instruction-tuned Gemma3 model with 12B parameters. The system is based on the self-refine workflow \\citet{madaan_selfrefine_2023} adapted for the WMT-25 machine translation tasks. No training data and model adaptation was used, as the goal of the approach was to test the viability of using a capable smaller off-the-shelf LLM in combination with a simple agentic system.\r\n\r\nBoth the self-refine system and the simple translator (both using a single instance of Gemma3 with 12B parameters) were used to translate all the texts of the WMT25 dataset (both the \"general\" and the \"multilingual\" pairs), and the results were evaluated using MetricX-24-XL. All the translations were performed and evaluated on the paragraph level. The scores demonstrated that the self-refine system resulted in competitive or slightly improved scores for the majority of language pairs. Therefore, the submitted results were produced by the described self-refine system.\r\n\r\n@inproceedings{madaan_selfrefine_2023,\r\n author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},\r\n booktitle = {Advances in Neural Information Processing Systems},\r\n editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},\r\n pages = {46534--46594},\r\n publisher = {Curran Associates, Inc.},\r\n title = {Self-Refine: Iterative Refinement with Self-Feedback},\r\n volume = {36},\r\n year = {2023}\r\n}",
    "architecture": "LLM multi agent system",
    "base_model": "Gemma-3-12b-it",
    "parameter_count": "12",
    "translation_granularity": "paragraph",
    "post_editing": "Reasoning, Prompt engineering, On \"self-refine\" iteration with reasoning",
    "is_multimodal": "NO",
    "data_preparation": "no training data, ie, no model adaptation, was used",
    "model_manipulation": "no training data, ie, no model adaptation, was used",
    "comments": "As for the language coverage, our experiments indicated that gemma-3-12b-it has decent performance across a variety of languages, and decided to apply it for every language pair. So \"primary languages\" should be understood in this sense - no fine-tuning was performed.",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "supported",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Kaze-MT": {
    "constrained": false,
    "description": "This paper describes the Kaze-MT submission to the WMT-25 General MT task for the Japanese-Chinese track. The system relies on a minimalist Test-Time-Scaling pipeline: Sampling - Scoring - Selection. 1. Sampling: We utilize zero-shot Qwen 2.5 models (72B and 14B) 512 times under a fixed temperature schedule to produce a diverse set of candidate translations; 2. Scoring: Quality Estimation models (COMETKiwi, MetricX, and ReMedy) are then used to score every candidate to get its quality scores; 3. Selection: Finally, we select the best candidates based on a majority voting strategy using their QE scores. Our submission requires no fine-tuning, in-context examples, or specialised decoding heuristics. We participate in both unconstrained and constrained tracks.",
    "architecture": "LLMs",
    "base_model": "Qwen2.5-72B",
    "parameter_count": "72",
    "translation_granularity": "paragraph",
    "post_editing": "Quality estimation, Ensemble",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "ja-zh_CN": "supported"
    }
  },
  "KIKIS": {
    "constrained": true,
    "description": "We participated in the constrained English–Japanese track of the WMT 2025 General Machine Translation Task.\r\nOur system collected the outputs produced by multiple subsystems, each consisting of LLM-based translation and re-ranking models configured differently (e.g., prompting strategies and context lengths), and then re-ranked those outputs.\r\nEach subsystem generated multiple segment-level candidates and iteratively selected the most probable one to construct the document translation.\r\nWe then reranked the document-level outputs from all subsystems to obtain the final translation.\r\nFor reranking, we adopted a text-based LLM reranking approach with a reasoning model to take long context into account.\r\nAdditionally, we built a bilingual dictionary on the fly from the parallel corpus to make the system more robust to rare words.",
    "architecture": "LLMs",
    "base_model": "plamo-2-translate",
    "parameter_count": "17.7",
    "translation_granularity": "mixed",
    "post_editing": "Reasoning, Ensemble, Prompt engineering",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...)",
    "model_manipulation": "unknown",
    "supported_lps": {
      "en-ja_JP": "supported"
    }
  },
  "KYUoM": {
    "constrained": false,
    "description": "KYUoM submission is a two-stage model equipped with gated graph-based fusion mechanisms for transferring visual knowledge with pre-trained models for\r\nmultimodal machine translation and traditional machine translation. The best improvement was achieved thanks to two-stage continuous learning with multimodal scene graphs and linguistic scene graphs.",
    "architecture": "enc-dec",
    "base_model": "NLLB",
    "parameter_count": "633.2M",
    "translation_granularity": "sentence",
    "post_editing": "Quality estimation",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "en-uk_UA": "unknown"
    }
  },
  "Laniqo": {
    "constrained": true,
    "description": "The Laniqo's submission for the WMT25 General Translation Task uses EuroLLM-9B-Instruct as the base translation model. The approach involved creating a high-quality preference dataset by employing Minimum Bayes Risk (MBR) decoding using a subset of NewsPALM document-level data across various translation directions. This dataset was then used to train a LoRA adapter via Contrastive Preference Optimization. All previous WMT test sets, along with WMT24++, FLORES-200 and NTREX-128 were indexed in a vector search database. This database was queried to select few-shot examples for each prompt.  For the final translation output, the system first performs QE reranking, which is then followed by MBR decoding. Finally, rule-based post-processing steps were applied to refine the translations.",
    "architecture": "LLM, dec-only",
    "base_model": "EuroLLM-9B-Instruct",
    "parameter_count": "9",
    "translation_granularity": "paragraph, mixed",
    "post_editing": "Minimum Risk Bayes decoding, Quality estimation, Rule-based post-processing",
    "is_multimodal": "NO",
    "data_preparation": "Synthetic data (back-translation, other)",
    "model_manipulation": "contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-cs_CZ": "supported",
      "en-et_EE": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-ko_KR": "supported",
      "en-ru_RU": "supported",
      "en-uk_UA": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Erlendur": {
    "constrained": false,
    "description": "Erlendur is a multilingual LLM-based translation system that uses a multi-stage pipeline with preparatory text analysis, dictionary lookup, and glossary integration, with a focus on English-to-Icelandic translation. The system's language-agnostic features address LLM limitations for lower-resource languages through hybrid prompting and post-processing with grammatical error correction.",
    "architecture": "hybrid LLM-approach",
    "base_model": "Claude 3.5 Sonnet",
    "parameter_count": "unk",
    "translation_granularity": "document (default in blindset)",
    "post_editing": "Automatic post editing (e.g. LLM)",
    "is_multimodal": "NO",
    "data_preparation": "More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI)",
    "model_manipulation": "unknown",
    "supported_lps": {
      "en-is_IS": "supported"
    }
  },
  "IR-MultiagentMT": {
    "constrained": false,
    "description": "\\textsc{solo}-MultiagentMT's submission comprises a multi-agent translation model founded on the GPT series \\citep{kim:25:ir-multiagent-mt}. Leveraging our recent MQM model, designated as Rubric-MQM, which mitigates the labeling bias inherent in Gemba-MQM while enhancing its precision in detection, we employ the Prompt Chaining workflow. Within this framework, we deploy three agents: the Translation Agent, the Post-edit Agent, and the Proofread Agent. The Translation Agent is responsible for generating the initial translation in accordance with the officially supplied prompt. The Post-edit Agent identifies errors in the translation and classifies them using the MQM framework with severities from 1 to 4. An important feature of our agent is that it effectively post-edits by proposing improved translations for each identified segment. The Proofread Agent, utilizing the post-edited translation, enhances the outcome by rephrasing it in five different versions while taking into account fluency and adequacy, ultimately selecting the optimal translation. We submit two versions for the unconstrained track: the first, derived from the Post-edit Agent, and the second, from the Proofread Agent. All results are obtained using the GPT-4o-mini (2024-07-18), which incurs lower costs compared to GPT-4o or o3, with a temperature setting of 1 and a maximum token limit of 1024. Our experimental findings indicate that the greatest improvement is observed when the Post-edit Agent is incorporated into the process, particularly when the initial translation quality is high. Additionally, by achieving comparable results with a more cost-effective model, we propose a methodology for a self-improving machine translation model.\r\n@inproceedings{kim:25:ir-multiagent-mt,\r\n    title=\"IR-MultiagentMT at WMT 25 Translation Task\",\r\n    year=\"2025\",\r\n    booktitle=\"Proceedings of the Ninth Conference on Machine Translation(WMT)\",\r\n    publisher=\"Association for Computational Linguistics\"\r\n}",
    "architecture": "LLM",
    "base_model": "GPT-4o-mini",
    "parameter_count": "-",
    "translation_granularity": "mixed",
    "post_editing": "Prompt engineering",
    "is_multimodal": "NO",
    "data_preparation": "More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI)",
    "model_manipulation": "prompt engineering",
    "comments": "applying prompt chaining workflow of agentic AI",
    "supported_lps": {
      "cs-de_DE": "unsupported",
      "cs-uk_UA": "unsupported",
      "en-ar_EG": "unsupported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "unsupported",
      "en-de_DE": "unsupported",
      "en-el_GR": "unsupported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "unsupported",
      "en-id_ID": "unsupported",
      "en-is_IS": "unsupported",
      "en-it_IT": "unsupported",
      "en-ja_JP": "unsupported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "unsupported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "unsupported",
      "en-ru_RU": "unsupported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "unsupported",
      "en-uk_UA": "unsupported",
      "en-vi_VN": "unsupported",
      "en-zh_CN": "unsupported",
      "ja-zh_CN": "unsupported"
    }
  },
  "NTTSU": {
    "constrained": true,
    "description": "This paper presents the submission of NTTSU for the constrained track of the English-Japanese and Japanese-Chinese at the WMT2025 General Machine Translation Task.\r\nFor each language pair, we trained three large language models (LLMs), with various settings, e.g., whether to use continual pre-training, supervised fine-tuning, preference optimization based on the quality and adequacy, and model merging.\r\nWe generated translation candidates for each model using context-aware MBR decoding.",
    "architecture": "LLMs",
    "base_model": "Qwen3-14B",
    "parameter_count": "14B",
    "translation_granularity": "mixed",
    "post_editing": "Minimum Risk Bayes decoding",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "supported_lps": {
      "en-ja_JP": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Wenyiil": {
    "constrained": true,
    "description": "Our submission introduces a state-of-the-art translation system, Wenyiil, built upon a sophisticated Large Language Model (LLM). The model's exceptional performance is achieved through a comprehensive training pipeline, featuring multi-stage supervised fine-tuning (SFT) on translation tasks and a two-stage post-training regimen. To overcome the limitations of standard decoding, we employ a novel hybrid decoding strategy that integrates word alignment with an advanced Minimum Bayes Risk (MBR) reranking algorithm. This approach not only enhances translation stability but also ensures superior accuracy across diverse linguistic contexts.",
    "architecture": "LLMs",
    "base_model": "qwen3",
    "parameter_count": "14",
    "translation_granularity": "mixed",
    "post_editing": "Minimum Risk Bayes decoding, Quality estimation, Ensemble, Automatic post editing (e.g. LLM)",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-cs_CZ": "supported",
      "en-et_EE": "supported",
      "en-ja_JP": "supported",
      "en-ko_KR": "supported",
      "en-ru_RU": "supported",
      "en-sr_Latn_RS": "supported",
      "en-uk_UA": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "ctpc_nlp": {
    "constrained": true,
    "description": "CTPC_NLP team submition a paper on nllb(600M) as encoder, llama(8B) as decoder , use adapter concat them.",
    "supported_lps": {
      "en-cs_CZ": "unknown"
    }
  },
  "Shy": {
    "constrained": true,
    "description": "Shy submission is based on LLM. More is on the way",
    "architecture": "LLM",
    "base_model": "Hunyuan 7b dense model",
    "parameter_count": "7b",
    "translation_granularity": "sentence, paragraph, document (default in blindset), mixed",
    "post_editing": "Prompt engineering",
    "is_multimodal": "Video or audio for speech domain",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, GRPO",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "SRPOL": {
    "constrained": true,
    "description": "SRPOL submission in its innovative approach is using an LLM with A-star decoding technique (instead of usually employed greedy, beam or sampling techniques) and subsequent reranking of the hypotheses based on Comet-QE and NLLB that together gives significant improvement of translation quality. EuroLLM as an LLM model for translation was used.",
    "architecture": "hybrid",
    "base_model": "EuroLLM, NLLB",
    "parameter_count": "12",
    "translation_granularity": "mixed",
    "post_editing": "Quality estimation, Ensemble",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "unsupported",
      "en-cs_CZ": "supported",
      "en-et_EE": "supported",
      "en-ja_JP": "unsupported",
      "en-ru_RU": "supported",
      "en-uk_UA": "supported",
      "en-zh_CN": "unsupported",
      "ja-zh_CN": "unsupported"
    }
  },
  "Systran": {
    "constrained": true,
    "description": "We present an English-to-Japanese translation system built upon the EuroLLM-9B\\cite{martins2025eurollm9btechnicalreport} model. The training process involves two main stages: continue pretraining (CPT) and supervised fine-tuning (SFT). After both stages, we further tuned the model using a development set to optimize performance. For training data, we employed both basic filtering techniques and high-quality filtering strategies to ensure data cleanness. Additionally, we classify both the training data and development data into 4 different domains and we train and finetune with domain specific prompts during system training. Finally, we applied Minimum Bayes Risk (MBR) decoding and paragraph-level reranking for post-processing to enhance translation quality.\r\n\r\n@misc{martins2025eurollm9btechnicalreport,\r\n      title={EuroLLM-9B: Technical Report}, \r\n      author={Pedro Henrique Martins and João Alves and Patrick Fernandes and Nuno M. Guerreiro and Ricardo Rei and Amin Farajian and Mateusz Klimaszewski and Duarte M. Alves and José Pombal and Nicolas Boizard and Manuel Faysse and Pierre Colombo and François Yvon and Barry Haddow and José G. C. de Souza and Alexandra Birch and André F. T. Martins},\r\n      year={2025},\r\n      eprint={2506.04079},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.CL},\r\n      url={https://arxiv.org/abs/2506.04079}, \r\n}",
    "architecture": "LLM",
    "base_model": "EuroLLM-9B",
    "parameter_count": "18B",
    "translation_granularity": "sentence",
    "post_editing": "Minimum Risk Bayes decoding, Quality estimation, Ensemble, Prompt engineering",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI)",
    "model_manipulation": "supervised finetuning",
    "supported_lps": {
      "en-ja_JP": "supported"
    }
  },
  "TranssionMT": {
    "constrained": false,
    "description": "The TranssionMT team employs the Transformer architecture and finetuning the translation of a specific language within the multilingual pretrained model to enhance its translation performance. Additionally, the team adopts various strategies, such as finetuning language model instructions, joint training of similar languages, integrated model decision - making, and non - English data mining, all aimed at improving the translation outcomes.",
    "architecture": "enc-dec、LLM",
    "base_model": "NLLB、EuroLLM",
    "parameter_count": "1.3B",
    "translation_granularity": "sentence",
    "post_editing": "Ensemble",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, adaptions (e.g. LoRA)",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-cs_CZ": "supported",
      "en-et_EE": "supported",
      "en-mas_KE": "supported",
      "en-ru_RU": "supported",
      "en-sr_Latn_RS": "supported",
      "en-uk_UA": "supported"
    }
  },
  "TranssionTranslate": {
    "constrained": false,
    "description": "Machine translation system for the ​WMT25 Shared Task. Focused on ​low-resource languages​ and key global languages (English, Hindi), our approach integrates:\r\n\r\n​Hybrid Architecture: Combines ​neural MT (NMT)​​ with ​rule-based post-editing​ to handle morphologically rich languages.\r\n​Data Augmentation: Leverages ​back-translation​ and ​synthetic parallel corpora​ for under-resourced language pairs.\r\n​Domain Adaptation: Optimizes for ​localized content​ (e.g., SMS, social media) using user data.",
    "supported_lps": {
      "cs-de_DE": "unknown",
      "cs-uk_UA": "unknown",
      "en-ar_EG": "unknown",
      "en-bho_IN": "unknown",
      "en-bn_BD": "unknown",
      "en-cs_CZ": "unknown",
      "en-de_DE": "unknown",
      "en-el_GR": "unknown",
      "en-et_EE": "unknown",
      "en-fa_IR": "unknown",
      "en-hi_IN": "unknown",
      "en-id_ID": "unknown",
      "en-is_IS": "unknown",
      "en-it_IT": "unknown",
      "en-ja_JP": "unknown",
      "en-kn_IN": "unknown",
      "en-ko_KR": "unknown",
      "en-lt_LT": "unknown",
      "en-mr_IN": "unknown",
      "en-ro_RO": "unknown",
      "en-ru_RU": "unknown",
      "en-sr_Cyrl_RS": "unknown",
      "en-sr_Latn_RS": "unknown",
      "en-sv_SE": "unknown",
      "en-th_TH": "unknown",
      "en-tr_TR": "unknown",
      "en-uk_UA": "unknown",
      "en-vi_VN": "unknown",
      "en-zh_CN": "unknown",
      "ja-zh_CN": "unknown"
    }
  },
  "UvA-MT": {
    "constrained": false,
    "description": "UvA-MT's submission is based on the gemma_12b model, with a very light-weight post-training method, i.e., translation calibration.  The training set covers all general translation directions.",
    "architecture": "LLMs",
    "base_model": "Gemma3-12B",
    "parameter_count": "12B",
    "translation_granularity": "paragraph",
    "post_editing": "Purely beam searching",
    "is_multimodal": "NO",
    "data_preparation": "Synthetic data (back-translation, other)",
    "model_manipulation": "Calibration Method: https://arxiv.org/abs/2504.19044",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "unknown",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "unknown",
      "en-et_EE": "supported",
      "en-fa_IR": "unknown",
      "en-hi_IN": "unknown",
      "en-id_ID": "unknown",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unknown",
      "en-ko_KR": "supported",
      "en-lt_LT": "unknown",
      "en-mas_KE": "supported",
      "en-mr_IN": "unknown",
      "en-ro_RO": "unknown",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "unknown",
      "en-th_TH": "unknown",
      "en-tr_TR": "unknown",
      "en-uk_UA": "supported",
      "en-vi_VN": "unknown",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "COILD-BHO": {
    "constrained": true,
    "description": "We present an English-to-Bhojpuri machine translation system built using large language models (LLMs) with a two-stage approach: pretraining followed by instruction fine-tuning. The model is first pretrained on multilingual and Bhojpuri-related corpora to enhance language understanding. It is then fine-tuned using translation-specific instructions and prompts to adapt to Bhojpuri’s linguistic characteristics. Our results show significant improvements in translation quality, demonstrating the effectiveness of instruction tuning for low-resource language pairs like English–Bhojpuri.",
    "architecture": "LLM",
    "base_model": "Llama",
    "parameter_count": "7B",
    "translation_granularity": "document (default in blindset)",
    "post_editing": "Automatic post editing (e.g. LLM)",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...)",
    "model_manipulation": "supervised finetuning, adaptions (e.g. LoRA)",
    "supported_lps": {
      "en-bho_IN": "supported"
    }
  },
  "Yandex": {
    "constrained": false,
    "description": "This paper describes Yandex submission to the WMT25 General Machine Translation task. We participate in English-to-Russian translation direction and propose a purely LLM-based translation model.\r\nOur training procedure comprises a training pipeline of several stages built upon YandexGPT, an in-house general-purpose LLM.\r\nIn particular, firstly, we employ continual pretraining (post-pretrain) for MT task similar to Alves et.al., 2023 for initial adaptation to multilinguality and translation. Subsequently, we use SFT on parallel document-level corpus in the form of P-Tuning v2 (Lui et.al., 2021).\r\nFollowing SFT, we propose a novel alignment scheme of two stages, the first one being a curriculum learning with difficulty schedule and a second one - training the model for tag preservation and error correction with human post-edits as training samples.\r\nOur model achieves results comparable to human reference translations on multiple domains.",
    "architecture": "decoder-only LLM",
    "base_model": "YandexGPT",
    "parameter_count": "several billion",
    "translation_granularity": "multi-paragraph",
    "post_editing": "no post-editing",
    "is_multimodal": "NO",
    "data_preparation": "Basic data filtering (OPUS cleaner / empirical / ...), More elaborate data filtering (e.g. quality estimation, LLMs, COMET KIWI), Synthetic data (back-translation, other)",
    "model_manipulation": "supervised finetuning, contrastive finetuning (CPO, DPO), adaptions (e.g. LoRA)",
    "supported_lps": {
      "en-ru_RU": "supported"
    }
  },
  "AyaExpanse-32B": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "AyaExpanse-32B",
    "parameter_count": "32",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2412.04261",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "AyaExpanse-8B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "AyaExpanse-8B",
    "parameter_count": "8",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2412.04261",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Claude-4": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "Claude-4",
    "parameter_count": "unk",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://www.anthropic.com/news/claude-4",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "unknown",
      "en-ar_EG": "supported",
      "en-bho_IN": "unknown",
      "en-bn_BD": "supported",
      "en-cs_CZ": "unknown",
      "en-de_DE": "supported",
      "en-el_GR": "unknown",
      "en-et_EE": "unknown",
      "en-fa_IR": "unknown",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "unknown",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unknown",
      "en-ko_KR": "supported",
      "en-lt_LT": "unknown",
      "en-mas_KE": "unknown",
      "en-mr_IN": "unknown",
      "en-ro_RO": "unknown",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unknown",
      "en-sr_Latn_RS": "unknown",
      "en-sv_SE": "unknown",
      "en-th_TH": "unknown",
      "en-tr_TR": "supported",
      "en-uk_UA": "unknown",
      "en-vi_VN": "unknown",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "CommandA": {
    "constrained": false,
    "description": "",
    "architecture": "LLMs",
    "base_model": "Command-A",
    "parameter_count": "111",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "DeepSeek-V3": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "DeepSeek-V3",
    "parameter_count": "37A 671B",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
    "supported_lps": {
      "cs-de_DE": "unknown",
      "cs-uk_UA": "unknown",
      "en-ar_EG": "unknown",
      "en-bho_IN": "unknown",
      "en-bn_BD": "unknown",
      "en-cs_CZ": "unknown",
      "en-de_DE": "unknown",
      "en-el_GR": "unknown",
      "en-et_EE": "unknown",
      "en-fa_IR": "unknown",
      "en-hi_IN": "unknown",
      "en-id_ID": "unknown",
      "en-is_IS": "unknown",
      "en-it_IT": "unknown",
      "en-ja_JP": "unknown",
      "en-kn_IN": "unknown",
      "en-ko_KR": "unknown",
      "en-lt_LT": "unknown",
      "en-mas_KE": "unknown",
      "en-mr_IN": "unknown",
      "en-ro_RO": "unknown",
      "en-ru_RU": "unknown",
      "en-sr_Cyrl_RS": "unknown",
      "en-sr_Latn_RS": "unknown",
      "en-sv_SE": "unknown",
      "en-th_TH": "unknown",
      "en-tr_TR": "unknown",
      "en-uk_UA": "unknown",
      "en-vi_VN": "unknown",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Gemini-2.5-Pro": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "Gemini-2.5-Pro",
    "parameter_count": "unk",
    "translation_granularity": "document (default in blindset)",
    "post_editing": "Reasoning",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unknown",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Gemma-3-12B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "Gemma-3-12B",
    "parameter_count": "12",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2503.19786",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unknown",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unknown",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Gemma-3-27B": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "Gemma-3-27B",
    "parameter_count": "27",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2503.19786",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unknown",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unknown",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Llama-4-Maverick": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "Llama-4-Maverick",
    "parameter_count": "17A 400B",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "ONLINE-B": {
    "constrained": false,
    "description": "",
    "architecture": "unk",
    "base_model": "unk",
    "parameter_count": "unk",
    "translation_granularity": "document (default in blindset)",
    "post_editing": "unk",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "ONLINE-G": {
    "constrained": false,
    "description": "",
    "architecture": "unk",
    "base_model": "unk",
    "parameter_count": "unk",
    "translation_granularity": "document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "ONLINE-W": {
    "constrained": false,
    "description": "",
    "supported_lps": {
      "cs-de_DE": "unknown",
      "cs-uk_UA": "unknown",
      "en-ar_EG": "unknown",
      "en-cs_CZ": "unknown",
      "en-de_DE": "unknown",
      "en-el_GR": "unknown",
      "en-et_EE": "unknown",
      "en-id_ID": "unknown",
      "en-it_IT": "unknown",
      "en-ja_JP": "unknown",
      "en-ko_KR": "unknown",
      "en-lt_LT": "unknown",
      "en-ro_RO": "unknown",
      "en-ru_RU": "unknown",
      "en-sv_SE": "unknown",
      "en-tr_TR": "unknown",
      "en-uk_UA": "unknown",
      "en-zh_CN": "unknown",
      "ja-zh_CN": "unknown"
    }
  },
  "Qwen3-235B": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "Qwen3-235B",
    "parameter_count": "22A 235B",
    "translation_granularity": "paragraph",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2505.09388",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "CommandR7B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "Command-R7B",
    "parameter_count": "7",
    "translation_granularity": "paragraph",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2504.00698",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "unsupported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "GPT-4.1": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "unk",
    "parameter_count": "unk",
    "translation_granularity": "document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://help.openai.com/en/articles/8357869-how-to-change-your-language-setting-in-chatgpt",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unknown",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unknown",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Llama-3.1-8B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "Llama-3.1-8B",
    "parameter_count": "8",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://huggingface.co/meta-llama/Llama-3.1-8B",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "unsupported",
      "en-ar_EG": "unsupported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "unsupported",
      "en-de_DE": "supported",
      "en-el_GR": "unsupported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "supported",
      "en-id_ID": "unsupported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "unsupported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "unsupported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "unsupported",
      "en-ru_RU": "unsupported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "supported",
      "en-tr_TR": "unsupported",
      "en-uk_UA": "unsupported",
      "en-vi_VN": "unsupported",
      "en-zh_CN": "unsupported",
      "ja-zh_CN": "unsupported"
    }
  },
  "Mistral-7B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "Mistral-7B",
    "parameter_count": "7",
    "translation_granularity": "paragraph",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "unsupported",
      "cs-uk_UA": "unsupported",
      "en-ar_EG": "unsupported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "unsupported",
      "en-de_DE": "unsupported",
      "en-el_GR": "unsupported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "unsupported",
      "en-id_ID": "unsupported",
      "en-is_IS": "unsupported",
      "en-it_IT": "unsupported",
      "en-ja_JP": "unsupported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "unsupported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "unsupported",
      "en-ru_RU": "unsupported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "unsupported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "unsupported",
      "en-uk_UA": "unsupported",
      "en-vi_VN": "unsupported",
      "en-zh_CN": "unsupported",
      "ja-zh_CN": "unsupported"
    }
  },
  "Qwen2.5-7B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "Qwen2.5-7B",
    "parameter_count": "7b",
    "translation_granularity": "paragraph",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "unknown",
      "en-ar_EG": "supported",
      "en-bho_IN": "unknown",
      "en-bn_BD": "unknown",
      "en-cs_CZ": "unknown",
      "en-de_DE": "supported",
      "en-el_GR": "unknown",
      "en-et_EE": "unknown",
      "en-fa_IR": "unknown",
      "en-hi_IN": "unknown",
      "en-id_ID": "unknown",
      "en-is_IS": "unknown",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unknown",
      "en-ko_KR": "supported",
      "en-lt_LT": "unknown",
      "en-mas_KE": "unknown",
      "en-mr_IN": "unknown",
      "en-ro_RO": "unknown",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unknown",
      "en-sr_Latn_RS": "unknown",
      "en-sv_SE": "unknown",
      "en-th_TH": "supported",
      "en-tr_TR": "unknown",
      "en-uk_UA": "unknown",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "Mistral-Medium": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "Mistral-Medium",
    "parameter_count": "unk",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://mistral.ai/news/mistral-medium-3",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "unknown",
      "en-ar_EG": "supported",
      "en-bho_IN": "unknown",
      "en-bn_BD": "unknown",
      "en-cs_CZ": "unknown",
      "en-de_DE": "supported",
      "en-el_GR": "unknown",
      "en-et_EE": "unknown",
      "en-fa_IR": "unknown",
      "en-hi_IN": "unknown",
      "en-id_ID": "unknown",
      "en-is_IS": "unknown",
      "en-it_IT": "unknown",
      "en-ja_JP": "unknown",
      "en-kn_IN": "unknown",
      "en-ko_KR": "unknown",
      "en-lt_LT": "unknown",
      "en-mr_IN": "unknown",
      "en-ro_RO": "unknown",
      "en-sv_SE": "unknown",
      "en-th_TH": "unknown",
      "en-tr_TR": "unknown",
      "en-uk_UA": "unknown",
      "en-vi_VN": "unknown",
      "en-zh_CN": "unknown",
      "ja-zh_CN": "unknown"
    }
  },
  "TowerPlus-9B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "TowerPlus-9B",
    "parameter_count": "9",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2506.17080",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "unsupported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "unsupported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "supported",
      "en-id_ID": "unsupported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "supported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "unsupported",
      "en-uk_UA": "supported",
      "en-vi_VN": "unsupported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "TowerPlus-72B": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "TowerPlus-72B",
    "parameter_count": "72",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://arxiv.org/pdf/2506.17080",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "unsupported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "unsupported",
      "en-et_EE": "unsupported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "supported",
      "en-id_ID": "unsupported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "unsupported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "supported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "unsupported",
      "en-uk_UA": "supported",
      "en-vi_VN": "unsupported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "EuroLLM-9B": {
    "constrained": true,
    "description": "",
    "architecture": "llm",
    "base_model": "EuroLLM-9B",
    "parameter_count": "9",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://huggingface.co/utter-project/EuroLLM-9B-Instruct",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "supported",
      "en-id_ID": "unsupported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "supported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "unsupported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "EuroLLM-22B": {
    "constrained": false,
    "description": "",
    "architecture": "llm",
    "base_model": "EuroLLM-22B",
    "parameter_count": "22",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "unsupported",
      "en-bn_BD": "unsupported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "unsupported",
      "en-hi_IN": "supported",
      "en-id_ID": "unsupported",
      "en-is_IS": "unsupported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "unsupported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "unsupported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "unsupported",
      "en-sr_Latn_RS": "unsupported",
      "en-sv_SE": "supported",
      "en-th_TH": "unsupported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "unsupported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  },
  "NLLB": {
    "constrained": true,
    "description": "",
    "architecture": "enc-dec",
    "base_model": "NLLB",
    "parameter_count": "0.7",
    "translation_granularity": "paragraph, document (default in blindset)",
    "post_editing": "",
    "is_multimodal": "NO",
    "data_preparation": "unknown",
    "model_manipulation": "unknown",
    "comments": "https://huggingface.co/facebook/nllb-200-distilled-600M",
    "supported_lps": {
      "cs-de_DE": "supported",
      "cs-uk_UA": "supported",
      "en-ar_EG": "supported",
      "en-bho_IN": "supported",
      "en-bn_BD": "supported",
      "en-cs_CZ": "supported",
      "en-de_DE": "supported",
      "en-el_GR": "supported",
      "en-et_EE": "supported",
      "en-fa_IR": "supported",
      "en-hi_IN": "supported",
      "en-id_ID": "supported",
      "en-is_IS": "supported",
      "en-it_IT": "supported",
      "en-ja_JP": "supported",
      "en-kn_IN": "supported",
      "en-ko_KR": "supported",
      "en-lt_LT": "supported",
      "en-mas_KE": "unsupported",
      "en-mr_IN": "supported",
      "en-ro_RO": "supported",
      "en-ru_RU": "supported",
      "en-sr_Cyrl_RS": "supported",
      "en-sr_Latn_RS": "supported",
      "en-sv_SE": "supported",
      "en-th_TH": "supported",
      "en-tr_TR": "supported",
      "en-uk_UA": "supported",
      "en-vi_VN": "supported",
      "en-zh_CN": "supported",
      "ja-zh_CN": "supported"
    }
  }
}
